2024-04-02 13:01:43,875 - INFO - Args parsed succesfully
2024-04-02 13:01:43,875 - INFO - Params: Namespace(emotion_transform_prob=0.0, epochs=100, exp_name='Regular_New_Transforms_No_Normalization', fine_tune=False, load_byol=False, net_name='dino_v2', use_expanded_dataset=False, verbose=True)
2024-04-02 13:01:44,081 - INFO - using MLP layer as FFN
2024-04-02 13:01:47,971 - INFO - Model was initialized and sent to cuda:0
2024-04-02 13:01:47,972 - INFO - EmotionNet(
  (nn_model): DinoVisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))
      (norm): Identity()
    )
    (blocks): ModuleList(
      (0-11): 12 x NestedTensorBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MemEffAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): LayerScale()
        (drop_path1): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (ls2): LayerScale()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=1024, out_features=7, bias=True)
  )
)
2024-04-02 13:01:50,147 - INFO - All data has been loaded
2024-04-02 13:01:50,149 - INFO - Got hyper parametrs for current model:
2024-04-02 13:01:50,149 - INFO - batch size=128
2024-04-02 13:01:50,149 - INFO - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0002
    lr: 0.0002
    maximize: False
    weight_decay: 2e-05
)
2024-04-02 13:01:50,149 - INFO - <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f51523c2d90>
2024-04-02 13:01:50,149 - INFO - DataLoaders initialized
2024-04-02 13:01:50,149 - INFO - HyperParams initiliazed, starting training...
